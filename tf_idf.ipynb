{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=['This is the first document.',\n",
    "        'This document is the second document',\n",
    "        'And this is the third one.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "vectorizer=TfidfVectorizer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46941728 0.61722732 0.3645444  0.         0.\n",
      "  0.3645444  0.         0.3645444 ]\n",
      " [0.         0.7284449  0.         0.28285122 0.         0.47890875\n",
      "  0.28285122 0.         0.28285122]\n",
      " [0.49711994 0.         0.         0.29360705 0.49711994 0.\n",
      "  0.29360705 0.49711994 0.29360705]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '100' '11' '12' '15' '1st' '20' '2day' '30' '4th' 'able' 'about'\n",
      " 'absolutely' 'account' 'actually' 'add' 'after' 'afternoon' 'again' 'ago'\n",
      " 'agree' 'ah' 'ahh' 'ahhh' 'aint' 'air' 'album' 'all' 'almost' 'alone'\n",
      " 'along' 'alot' 'already' 'alright' 'also' 'although' 'always' 'am'\n",
      " 'amazing' 'an' 'and' 'another' 'answer' 'any' 'anymore' 'anyone'\n",
      " 'anything' 'anyway' 'app' 'apparently' 'apple' 'appreciate' 'are' 'aren'\n",
      " 'around' 'as' 'ask' 'asleep' 'at' 'ate' 'aw' 'awake' 'away' 'awesome'\n",
      " 'aww' 'awww' 'babe' 'baby' 'back' 'bad' 'band' 'bank' 'bbq' 'bday' 'be'\n",
      " 'beach' 'beat' 'beautiful' 'because' 'bed' 'been' 'beer' 'before'\n",
      " 'behind' 'being' 'believe' 'best' 'bet' 'better' 'bgt' 'big' 'bike'\n",
      " 'birthday' 'bit' 'black' 'bless' 'blip' 'blog' 'blue' 'body' 'boo' 'book'\n",
      " 'bored' 'boring' 'both' 'bought' 'bout' 'boy' 'boyfriend' 'boys' 'break'\n",
      " 'breakfast' 'bring' 'bro' 'broke' 'broken' 'brother' 'brothers' 'btw'\n",
      " 'bummed' 'bus' 'business' 'busy' 'but' 'buy' 'by' 'bye' 'cake' 'call'\n",
      " 'called' 'came' 'camera' 'can' 'cannot' 'cant' 'car' 'card' 'care' 'case'\n",
      " 'cat' 'catch' 'cause' 'cd' 'chance' 'change' 'chat' 'check' 'cheese'\n",
      " 'chicken' 'chillin' 'chocolate' 'church' 'city' 'class' 'clean'\n",
      " 'cleaning' 'close' 'closed' 'clothes' 'club' 'coffee' 'cold' 'college'\n",
      " 'com' 'come' 'comes' 'coming' 'comment' 'company' 'completely' 'computer'\n",
      " 'concert' 'congrats' 'cool' 'cos' 'could' 'couldn' 'couple' 'course'\n",
      " 'crazy' 'cream' 'cry' 'crying' 'cup' 'cut' 'cute' 'cuz' 'da' 'dad'\n",
      " 'dance' 'dang' 'dark' 'date' 'david' 'day' 'days' 'dead' 'dear' 'decided'\n",
      " 'definitely' 'did' 'didn' 'didnt' 'die' 'died' 'different' 'dinner' 'dm'\n",
      " 'do' 'does' 'doesn' 'doesnt' 'dog' 'doing' 'don' 'done' 'dont' 'down'\n",
      " 'dream' 'dreams' 'dress' 'drink' 'drinking' 'drive' 'driving' 'drunk'\n",
      " 'dude' 'due' 'during' 'dvd' 'each' 'earlier' 'early' 'easy' 'eat'\n",
      " 'eating' 'either' 'else' 'em' 'email' 'end' 'english' 'enjoy' 'enjoying'\n",
      " 'enough' 'episode' 'especially' 'etc' 'even' 'evening' 'ever' 'every'\n",
      " 'everybody' 'everyone' 'everything' 'exactly' 'exam' 'exams' 'except'\n",
      " 'excited' 'exciting' 'eyes' 'face' 'facebook' 'fact' 'fail' 'fair' 'fall'\n",
      " 'family' 'fan' 'fans' 'fantastic' 'far' 'fast' 'favorite' 'fb' 'feel'\n",
      " 'feeling' 'feels' 'feet' 'fell' 'felt' 'few' 'ff' 'figure' 'film' 'final'\n",
      " 'finally' 'find' 'fine' 'fingers' 'finish' 'finished' 'first' 'fix'\n",
      " 'flight' 'flu' 'fly' 'fm' 'follow' 'followers' 'following' 'food' 'for'\n",
      " 'forever' 'forget' 'forgot' 'forward' 'found' 'free' 'french' 'friday'\n",
      " 'friend' 'friends' 'from' 'full' 'fun' 'funny' 'game' 'garden' 'gave'\n",
      " 'gd' 'get' 'gets' 'getting' 'gift' 'girl' 'girls' 'give' 'glad' 'go'\n",
      " 'god' 'goes' 'goin' 'going' 'gone' 'gonna' 'good' 'goodnight' 'google'\n",
      " 'got' 'gotta' 'graduation' 'great' 'green' 'group' 'guess' 'guitar'\n",
      " 'gutted' 'guy' 'guys' 'gym' 'ha' 'had' 'haha' 'hahah' 'hahaha' 'hair'\n",
      " 'half' 'hand' 'hang' 'hanging' 'happen' 'happened' 'happens' 'happy'\n",
      " 'hard' 'has' 'hate' 'hates' 'have' 'haven' 'havent' 'having' 'he' 'head'\n",
      " 'headache' 'heading' 'hear' 'heard' 'heart' 'hehe' 'hell' 'hello' 'help'\n",
      " 'her' 'here' 'hey' 'hi' 'high' 'him' 'his' 'hit' 'hmm' 'holiday' 'home'\n",
      " 'homework' 'hope' 'hopefully' 'hoping' 'horrible' 'hospital' 'hot' 'hour'\n",
      " 'hours' 'house' 'how' 'http' 'hubby' 'hug' 'huge' 'hugs' 'huh' 'hun'\n",
      " 'hungry' 'hurt' 'hurts' 'ice' 'id' 'idea' 'idk' 'if' 'ill' 'im' 'in'\n",
      " 'inside' 'instead' 'interesting' 'internet' 'into' 'iphone' 'ipod' 'is'\n",
      " 'isn' 'isnt' 'it' 'its' 'ive' 'i√Ø' 'jealous' 'job' 'john' 'join' 'jonas'\n",
      " 'july' 'june' 'just' 'justin' 'keep' 'keeps' 'kid' 'kids' 'kind' 'kinda'\n",
      " 'knew' 'know' 'la' 'lady' 'lame' 'laptop' 'last' 'late' 'later' 'lazy'\n",
      " 'learn' 'least' 'leave' 'leaving' 'left' 'legs' 'less' 'let' 'lets'\n",
      " 'life' 'like' 'liked' 'lil' 'line' 'link' 'list' 'listen' 'listening'\n",
      " 'little' 'live' 'living' 'll' 'lmao' 'lol' 'london' 'lonely' 'long'\n",
      " 'longer' 'look' 'looked' 'looking' 'looks' 'lost' 'lot' 'lots' 'love'\n",
      " 'loved' 'lovely' 'loves' 'loving' 'low' 'luck' 'lucky' 'lunch' 'luv' 'ly'\n",
      " 'ma' 'mac' 'mad' 'made' 'mail' 'make' 'makes' 'making' 'mama' 'man'\n",
      " 'many' 'may' 'maybe' 'me' 'mean' 'means' 'meant' 'meet' 'meeting' 'men'\n",
      " 'message' 'messages' 'met' 'middle' 'might' 'miles' 'min' 'mind' 'mine'\n",
      " 'minutes' 'miss' 'missed' 'missing' 'mobile' 'mom' 'moment' 'mommy'\n",
      " 'moms' 'monday' 'money' 'month' 'months' 'mood' 'moon' 'more' 'morning'\n",
      " 'most' 'mother' 'mothers' 'move' 'moved' 'movie' 'movies' 'moving' 'mr'\n",
      " 'much' 'mum' 'music' 'must' 'my' 'myself' 'myspace' 'name' 'nap' 'near'\n",
      " 'need' 'needed' 'needs' 'net' 'never' 'new' 'news' 'next' 'nice' 'night'\n",
      " 'nite' 'no' 'non' 'none' 'nope' 'not' 'nothing' 'now' 'number' 'of' 'off'\n",
      " 'office' 'officially' 'oh' 'ohh' 'ok' 'okay' 'old' 'omg' 'on' 'once'\n",
      " 'one' 'ones' 'online' 'only' 'ooh' 'open' 'or' 'other' 'others' 'ouch'\n",
      " 'our' 'out' 'outside' 'over' 'own' 'packing' 'page' 'pain' 'paper'\n",
      " 'parents' 'park' 'part' 'party' 'pass' 'past' 'people' 'perfect' 'person'\n",
      " 'phone' 'photo' 'photos' 'pic' 'pick' 'pics' 'picture' 'pictures' 'pizza'\n",
      " 'place' 'plan' 'plans' 'play' 'played' 'playing' 'please' 'plurk' 'plus'\n",
      " 'point' 'pool' 'poor' 'post' 'posted' 'ppl' 'pretty' 'probably' 'problem'\n",
      " 'problems' 'profile' 'project' 'prom' 'put' 'quick' 'quite' 'radio'\n",
      " 'rain' 'raining' 'rainy' 'ran' 'rather' 're' 'read' 'reading' 'ready'\n",
      " 'real' 'realized' 'really' 'reason' 'red' 'relaxing' 'remember' 'reply'\n",
      " 'rest' 'revision' 'ride' 'right' 'rock' 'room' 'run' 'running' 'sad'\n",
      " 'sadly' 'safe' 'said' 'same' 'sat' 'saturday' 'save' 'saw' 'say' 'saying'\n",
      " 'says' 'school' 'screen' 'season' 'second' 'see' 'seeing' 'seem' 'seems'\n",
      " 'seen' 'send' 'sending' 'sent' 'seriously' 'set' 'shall' 'shame' 'share'\n",
      " 'she' 'shirt' 'shoes' 'shop' 'shopping' 'short' 'should' 'show' 'shower'\n",
      " 'shows' 'sick' 'side' 'sigh' 'sign' 'since' 'sing' 'single' 'sister'\n",
      " 'site' 'sitting' 'sleep' 'sleeping' 'sleepy' 'slept' 'slow' 'small'\n",
      " 'smile' 'snl' 'so' 'sold' 'some' 'someone' 'something' 'sometimes'\n",
      " 'somewhere' 'son' 'song' 'songs' 'soo' 'soon' 'sooo' 'soooo' 'sore'\n",
      " 'sorry' 'sound' 'sounds' 'special' 'spend' 'spent' 'star' 'start'\n",
      " 'started' 'starting' 'starts' 'stay' 'staying' 'still' 'stomach' 'stop'\n",
      " 'stopped' 'store' 'story' 'stuck' 'study' 'studying' 'stuff' 'stupid'\n",
      " 'such' 'suck' 'sucks' 'summer' 'sun' 'sunday' 'sunny' 'sunshine' 'super'\n",
      " 'support' 'supposed' 'sure' 'sweet' 'swine' 'take' 'taken' 'takes'\n",
      " 'taking' 'talk' 'talking' 'tea' 'team' 'tell' 'test' 'text' 'than'\n",
      " 'thank' 'thanks' 'that' 'thats' 'the' 'their' 'them' 'then' 'there'\n",
      " 'these' 'they' 'thing' 'things' 'think' 'thinking' 'thinks' 'this' 'tho'\n",
      " 'those' 'though' 'thought' 'three' 'throat' 'through' 'thx' 'tickets'\n",
      " 'til' 'till' 'time' 'times' 'tinyurl' 'tired' 'to' 'today' 'together'\n",
      " 'told' 'tom' 'tomorrow' 'tonight' 'too' 'took' 'top' 'totally' 'tour'\n",
      " 'town' 'traffic' 'train' 'trek' 'tried' 'trip' 'true' 'try' 'trying'\n",
      " 'tuesday' 'turn' 'tv' 'tweet' 'tweeting' 'tweets' 'twitpic' 'twitter'\n",
      " 'two' 'ugh' 'uk' 'under' 'understand' 'unfortunately' 'until' 'up'\n",
      " 'update' 'updates' 'upset' 'ur' 'us' 'use' 'used' 'using' 'usually' 've'\n",
      " 'vegas' 'version' 'very' 'via' 'video' 'videos' 'visit' 'vote' 'wait'\n",
      " 'waiting' 'wake' 'walk' 'walking' 'wanna' 'want' 'wanted' 'wants' 'warm'\n",
      " 'wars' 'was' 'wasn' 'watch' 'watched' 'watching' 'water' 'way' 'we'\n",
      " 'wear' 'weather' 'wedding' 'week' 'weekend' 'weeks' 'weird' 'welcome'\n",
      " 'well' 'went' 'were' 'what' 'whatever' 'whats' 'when' 'where' 'which'\n",
      " 'while' 'white' 'who' 'whole' 'why' 'wife' 'will' 'win' 'windows' 'wine'\n",
      " 'wish' 'wishes' 'wishing' 'with' 'without' 'woke' 'won' 'wonder'\n",
      " 'wonderful' 'wondering' 'wont' 'woo' 'word' 'words' 'work' 'worked'\n",
      " 'working' 'works' 'world' 'worry' 'worse' 'worst' 'worth' 'would'\n",
      " 'wouldn' 'wow' 'write' 'writing' 'wrong' 'wtf' 'www' 'xd' 'ya' 'yay'\n",
      " 'yea' 'yeah' 'year' 'years' 'yep' 'yes' 'yesterday' 'yet' 'yo' 'you'\n",
      " 'your' 'yours' 'yourself' 'youtube' 'yum' 'yummy' '¬Ωm']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "df['text']=df['text'].fillna('')\n",
    "vectorizer=TfidfVectorizer(max_features=1000)\n",
    "X=vectorizer.fit_transform(df['text'])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
